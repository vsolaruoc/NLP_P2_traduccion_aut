{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":464671,"sourceType":"datasetVersion","datasetId":213609},{"sourceId":12110026,"sourceType":"datasetVersion","datasetId":7624497}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"width: 100%; clear: both;\">\n<div style=\"float: left; width: 50%;\">\n<img src=\"https://www.uoc.edu/portal/_resources/common/imatges/sala_de_premsa/noticies/2016/202-nova-marca-uoc.jpg\", align=\"left\" width=\"380\" height=\"120\">\n\n</div>\n</div>\n<div style=\"float: right; width: 50%;\">\n<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.893 · Análisis de textos</p>\n<p style=\"margin: 0; text-align:right;\">Máster en Ciencia de Datos Aplicada</p>\n<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicaciones</p>\n</div>\n</div>\n<div style=\"width: 100%; clear: both;\">\n<div style=\"width:100%;\">&nbsp;</div>","metadata":{"id":"JkBKubyZ7na7"}},{"cell_type":"markdown","source":"# PRA 2: Deep Learning para el análisis de textos\n\nEn esta práctica revisaremos y aplicaremos los conocimientos aprendidos durante los últimos módulos del curso. En concreto trataremos los siguientes temas:\n\n1. **Traducción automática**: con custom embeddings y con embeddings preentrenados.\n2. **NER y NEL**: Entrenamiento de modelos de detección de entidades nombradas (NER), uso y clasificación.  Detección de entidades nombradas basándonos en Wikidata aplicada a NER.\n\nTambién incluimos algunos otros temas transversales trabajados a lo largo de la asignatura.\n","metadata":{"id":"qjWsCAwv7na-"}},{"cell_type":"markdown","source":"#0. Conexión con drive\n\nLa ejecución de esta sección es opcional, pero muy recomendable si se trabaja en Colab.\n\n\nAquí se realiza la conexión con drive y se establece el directorio de trabajo *actual*, en el que se almacenarán todos los recursos necesarios para ejecutar el notebook.\n\n\n\nEl path de trabajo se debería colocar en la variable `my_path_pra2` y se sugiere crear una estructura de directorios como la siguiente.\n\n**Estructura de directorios**\n\nEstablecer el directorio raiz según la variable `my_path_pra2`. En este directorio se almacenarán los datasets y directorios necesarios para la ejecución del notebook. La estructura y contenidos son los siguientes:\n\n    * directorio `TA` donde se almacenan los datos y recursos para realizar la traducción automática; contiene:\n      * glove.42B.300d.txt    # cargado por el usuario\n      * nld.txt      # cargado por el usuario\n      * directorio `model` donde se almacenan los *best model* del entrenamiento de los modelos de traducción automática:\n        * model_ta_en_de-g.keras    # 'best model' generado por el entrenamiento de TA con embeddings preentrenados\n        * model_ta_en_de.keras      # 'bestmodel' generado por el entrenamiento de TA con embeddings preentrenados\n    * directorio `NER` con los archivos necesarios para la práctica NER:\n        * Directorio `output_ner`   donde se almacena el *model-best* y *model-last* entrenados por este notebook\n        * config.cfg    # cargado por el usuario\n        * test.txt      # cargado por el usuario\n        * test.spacy    # Conversión de test.txt al formato spacy\n        * train.txt     # cargado por el usuario\n        * train.spacy   # Conversión de train.txt al formato spacy\n        * valid.txt     # cargado por el usuario\n        * valid.spacy   # Conversión de valid.txt al formato spacy\n\n\n\n**Ejecución de notebook en un entorn no `Colab`.**\n\nSi no se va a ejecutar este notebook en Colab, substituir esta sección (*0. Conexión on Drive*) por la correspondiente a la configuración deseada, teniendo en cuenta disponer de GPU con al menos 15 GB de memoria RAM.\n\n**Ejecucón de notebok en un entorno `Colab`.**\n\nSi se ejectua este notebook en Colab, debe utilizarse con al menos una GPU del tipo 'T4 GPU' o superior. Tener en cuenta que si se utiliza el servicio gratuito de Colab, estas GPU no están disponibles permanentemente y, cuando están disponibles, lo están solo mientras duran las 'compute units' asignadas al usuario o por límites de disponibilidad de GPUs de Google. Cuando éstas se agotan o no hay disponibilidad, debe esperarse a una nueva asignación. Google no publica el método de asignación o los [plazos de disposición](https://research.google.com/colaboratory/faq.html#usage-limits) de GPUs. La estructura de los directorios de trabajo debe ser la misma que la mencionada en el apartado anterior.","metadata":{"id":"wknR5cFPUe9P"}},{"cell_type":"code","source":"# Acceder a Colab myDrive\n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"nPDd3zbYVPz4","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:01:30.834583Z","iopub.execute_input":"2025-06-13T06:01:30.834933Z","iopub.status.idle":"2025-06-13T06:01:30.838829Z","shell.execute_reply.started":"2025-06-13T06:01:30.834888Z","shell.execute_reply":"2025-06-13T06:01:30.838276Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os","metadata":{"id":"sq7fCC0WVUTz","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:39:58.056983Z","iopub.execute_input":"2025-06-14T09:39:58.057212Z","iopub.status.idle":"2025-06-14T09:39:58.064066Z","shell.execute_reply.started":"2025-06-14T09:39:58.057187Z","shell.execute_reply":"2025-06-14T09:39:58.063273Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Establecer el directorio donde se han almacenado los ficheros de este ejercicio\n\n# Cambiar este directorio raiz si es necesario según la estructura de directorios de cada usuario\n# my_path_pra2 = \"/content/drive/MyDrive/UOC/Análisis_de_textos/PRA2/\"\n\n# if os.path.exists(my_path_pra2):\n#     try:\n#         os.chdir(my_path_pra2)\n#         print(f\"Directorio raíz cambiado a: '{os.getcwd()}'\")\n#     except Exception as e:\n#         print(f\"Error cambiando al directorio: '{my_path_pra2}'. Error: {e}\")\n# else:\n#     print(f\"Directorio '{my_path_pra2}' no existe\")","metadata":{"id":"pe3QszncVUzq","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:01:30.861551Z","iopub.execute_input":"2025-06-13T06:01:30.861746Z","iopub.status.idle":"2025-06-13T06:01:30.875401Z","shell.execute_reply.started":"2025-06-13T06:01:30.861730Z","shell.execute_reply":"2025-06-13T06:01:30.874755Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 1. Traducción Automática (TA) (7 puntos)\n\n\nEn esta primera parte de la práctica se pide resolver los ejercicios usando la libreria **KERAS**.","metadata":{"id":"kyW13h9Z7nbB"}},{"cell_type":"markdown","source":"## 1.1 TA con Custom Embeddings (4,5 puntos)","metadata":{"id":"JuKkXmmS7nbD"}},{"cell_type":"markdown","source":"\nEl objetivo de este apartado es entrenar un modelo de traducción automática de dos idiomas escogidos a partir del dataset elegido, siguiendo los mismos pasos que en el notebook de *Machine Translation* y el ejemplo proporcionado para el desarrollo de esta práctica `Ejemplo_PRA2`.","metadata":{"id":"FuhhJrUk7nbD"}},{"cell_type":"markdown","source":"<strong>Implementación:</strong> Siguiendo los pasos trabajados en el notebook de traducción automática, implementar y entrenar un modelo de traducción automática, del **idioma origen** a **idioma destino**. Para ello, considerar los siguientes aspectos: <br>\n    - Decidir que dimensión que se usará en la capa embedding. Se sugiere empezar con 200 y luego se pedirá variar este valor para comparar los resultados.<br>\n    - Plantear una longitud de secuencia que tenga sentido. Inicialmente, se pedirá trabajar con 8, y su valor se ajustará según la carga de procesamiento.<br>\n    - Mostrar la aplicación del modelo entrenado (predicción) con datos del dataset de test.<br>\n <br>","metadata":{"id":"Pdz0sUSb7nbD"}},{"cell_type":"markdown","source":"### 1.1.1 Preparación de datos (1 punto)","metadata":{"id":"1b3mC0ATqcAo"}},{"cell_type":"markdown","source":"Primero preparamos los datos que se han elegido (tened en cuenta que el idioma origen debe ser **inglés**), de tal manera que, se puedan leer correctamente y estén preparados para tenerlos en un formato adecuado para la práctica.","metadata":{"id":"L28axKt4kIwO"}},{"cell_type":"markdown","source":"**a. Cargamos los datos desde la fuente seleccionada.**\n\n*Salidas esperadas:*\n- Longitud del dataset.\n- Al menos 3 filas de datos en las que se muestre textos del idioma origen y la respectiva traducción.","metadata":{"id":"G0tpNpySqcAp"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\nimport numpy as np\nimport pandas as pd\nfrom numpy import array\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, RepeatVector\n\nfile_path = f\"/kaggle/input/data-eng-spa/data_eng_spa.tsv\"\n\neng_spa = pd.read_csv(file_path, sep=\"\\t\", header=None, usecols=[1, 3])\n\neng_spa = array(eng_spa)\n\nprint(\"Longitud del dataset:\", len(eng_spa))\n\n# Reducimos dataset\nn = len(eng_spa)\nrandom = np.random.RandomState(42)\nidx = random.choice(n, size=int(0.25*n), replace=False)\neng_spa = eng_spa[idx]\nprint(\"Longitud del dataset reducido:\", len(eng_spa))\n\neng_spa[:3]","metadata":{"id":"oVYldy9c7nbG","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:40:30.577339Z","iopub.execute_input":"2025-06-14T09:40:30.577983Z","iopub.status.idle":"2025-06-14T09:40:45.907263Z","shell.execute_reply.started":"2025-06-14T09:40:30.577957Z","shell.execute_reply":"2025-06-14T09:40:45.906707Z"}},"outputs":[{"name":"stderr","text":"2025-06-14 09:40:32.631997: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749894032.867301      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749894032.930099      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Longitud del dataset: 272015\nLongitud del dataset reducido: 68003\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"array([[\"It'd be better if you would come with me.\",\n        'Sería mejor si vinieras conmigo.'],\n       ['Women often paint their fingernails.',\n        'Las mujeres a menudo se pintan las uñas.'],\n       ['The climate of New Zealand is similar to that of Japan.',\n        'El clima de Nueva Zelanda es similar al de Japón.']],\n      dtype=object)"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"**b. Preprocesar los datos, para eliminar puntuaciones y poner en minúscula.**\n\n*Salida esperada:* Deberás mostrar un conjunto de datos limpio y normalizado. Por ejemplo, la frase en el idioma origen, \"Hello, world!\" se transformará en \"hello world\".","metadata":{"id":"6fmFddVqkYU_"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\n\nimport re\n\n# Preprocesamos los datos\npunctuation = '.;,\"!#$%&\\()*+-<>@[\\\\]^_`{|}~?'\n\n# Función para reemplazar ciertos signos de puntuación:\ndef clean_signs(text):\n    text = re.sub('[%s]' % re.escape(punctuation), ' ', text)\n    return text.lower()\n\n# Aplicar la función a datos vectorizados\nvec_clean = np.vectorize(clean_signs)\neng_spa = vec_clean(eng_spa)\nprint(eng_spa[:3])","metadata":{"id":"JgIsoQqS7nbH","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:01:46.919721Z","iopub.execute_input":"2025-06-13T06:01:46.919933Z","iopub.status.idle":"2025-06-13T06:01:47.985897Z","shell.execute_reply.started":"2025-06-13T06:01:46.919914Z","shell.execute_reply":"2025-06-13T06:01:47.984934Z"}},"outputs":[{"name":"stdout","text":"[[\"it'd be better if you would come with me \"\n  'sería mejor si vinieras conmigo ']\n ['women often paint their fingernails '\n  'las mujeres a menudo se pintan las uñas ']\n ['the climate of new zealand is similar to that of japan '\n  'el clima de nueva zelanda es similar al de japón ']]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"**c. Para tener una idea del tamaño de los textos a analizar, en función de la cantidad de palabras, visualizar los datos resultantes mediante un histograma.**\n\n*Salida esperada:* Dos histogramas que reflejen la cantidad de tokens de los textos del corpus, uno para los vectores del idioma origen y otro con los del destino.","metadata":{"id":"XRInE02Skhbv"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\nimport matplotlib.pyplot as plt\n# Visualizamos la longitud de los vectores del idioma origen\n\n# Distribución de longitudes de texto\ndef words_counter(text):\n    if type(text) != float:\n      return len(text.split()) # contar palabras de cada reseña\n    else:\n      return None\n\nlen_eng = [words_counter(text) for text in eng_spa[:,0] if words_counter(text) is not None]\nlen_spa = [words_counter(text) for text in eng_spa[:,1] if words_counter(text) is not None]\n\nprint(round(np.mean(len_eng),2))\nprint(round(np.mean(len_spa),2))\n\n# # Graficar histograma de longitud de textos:\nplt.hist(len_eng,bins=100)\nplt.title(\"Distribución de longitud de frases (inglés)\")\nplt.xlabel(\"Número de palabras\")\nplt.ylabel(\"Frecuencia\")\nplt.show()","metadata":{"id":"d8LfflHB7nbI","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T09:45:47.817315Z","iopub.execute_input":"2025-06-14T09:45:47.817872Z","iopub.status.idle":"2025-06-14T09:45:48.330840Z","shell.execute_reply.started":"2025-06-14T09:45:47.817845Z","shell.execute_reply":"2025-06-14T09:45:48.330188Z"}},"outputs":[{"name":"stdout","text":"6.82\n6.62\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\n\n# Visualizamos la longitud de los vectores del idioma destino\nplt.hist(len_spa,bins=100)\nplt.title(\"Distribución de longitud de frases (español)\")\nplt.xlabel(\"Número de palabras\")\nplt.ylabel(\"Frecuencia\")\nplt.show()","metadata":{"id":"QyM_eI7U7nbI","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:01:48.995353Z","iopub.execute_input":"2025-06-13T06:01:48.995522Z","iopub.status.idle":"2025-06-13T06:01:49.342000Z","shell.execute_reply.started":"2025-06-13T06:01:48.995509Z","shell.execute_reply":"2025-06-13T06:01:49.341325Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"Para la definición de los modelos, necesitamos instalar *keras* y *tensorflow*.","metadata":{"id":"wvz2u3zktH5s"}},{"cell_type":"code","source":"!pip install keras","metadata":{"id":"WPv_880u7nbJ","outputId":"d30cf1e0-70ff-47c6-b1bd-0c07d99f2dd2","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:01:49.342742Z","iopub.execute_input":"2025-06-13T06:01:49.342949Z","iopub.status.idle":"2025-06-13T06:01:53.216744Z","shell.execute_reply.started":"2025-06-13T06:01:49.342923Z","shell.execute_reply":"2025-06-13T06:01:53.216033Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (1.26.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.13.0)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.14.1)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->keras) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->keras) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->keras) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->keras) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->keras) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->keras) (2.4.1)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.13.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->keras) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->keras) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->keras) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->keras) (2024.2.0)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"id":"isjIX82C7nbK","outputId":"7962929a-ca05-43d4-9d30-047be0817a28","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:01:53.219382Z","iopub.execute_input":"2025-06-13T06:01:53.219841Z","iopub.status.idle":"2025-06-13T06:01:56.274242Z","shell.execute_reply.started":"2025-06-13T06:01:53.219816Z","shell.execute_reply":"2025-06-13T06:01:56.273512Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.0rc1)\nRequirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"**d. Calcular el vocabulario tanto en el idioma origen , como en el idioma destino, e imprimir su tamaño**.\n\n*Resultado esperado:* Se visualizarán dos números. Cada número representa el tamaño del vocabulario de las frases del idioma origen y del destino, respectivamente, después de haber aplicado el preprocesamiento y tokenización.","metadata":{"id":"JXqDyZiEks-S"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\n\nvocab_eng = [word for sentence in eng_spa[:,0] for word in sentence.split()]\nvocab_spa = [word for sentence in eng_spa[:,1] for word in sentence.split()]\n\nvocab_size_eng = len(set(vocab_eng))\nvocab_size_spa = len(set(vocab_spa))\n\nprint(f\"Tamaño idioma origen (eng): {vocab_size_eng}\")\nprint(f\"Tamaño idioma destino (spa): {vocab_size_spa}\")","metadata":{"id":"TTCjxsZMgj9E","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:01:56.275203Z","iopub.execute_input":"2025-06-13T06:01:56.275456Z","iopub.status.idle":"2025-06-13T06:01:56.891890Z","shell.execute_reply.started":"2025-06-13T06:01:56.275426Z","shell.execute_reply":"2025-06-13T06:01:56.891309Z"}},"outputs":[{"name":"stdout","text":"Tamaño idioma origen (eng): 17312\nTamaño idioma destino (spa): 28257\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**e. Separamos los conjuntos de entrenamiento por idioma y los codificamos.**\n\nEn este paso, se separarán los datos en dos conjuntos: uno para entrenamiento (llamado *train*) y otro para prueba (llamado *test*), utilizando una división del 80% para entrenamiento y 20% para prueba.\n\n\n*Salida esperada:* tres primeras filas del dataset de entrenamiento *train*.\n","metadata":{"id":"fPXEAlbGoSuL"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntrain, test = train_test_split(eng_spa, test_size=0.2, random_state = 12)\ntrain[:3]","metadata":{"id":"mX7rtG4Q7nbM","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:01:56.892680Z","iopub.execute_input":"2025-06-13T06:01:56.892950Z","iopub.status.idle":"2025-06-13T06:01:57.650132Z","shell.execute_reply.started":"2025-06-13T06:01:56.892926Z","shell.execute_reply":"2025-06-13T06:01:57.649507Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[\"there's too much sugar in the coffee \",\n        'el café tiene demasiado azúcar '],\n       [\"tom hasn't heard from mary since last june \",\n        'tom no ha oído de mary desde junio pasado '],\n       ['i was sure i had never seen her before ',\n        'yo estaba seguro de no haberla visto antes ']], dtype='<U1246')"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"**f. Definir y aplicar una función para codificar las secuencias**\n\nEn este paso, los dos conjuntos de datos creados en el punto anterior,  serán codificados usando **tokenización** y un proceso de **padding** para asegurar que todas las secuencias, de un mismo idioma, tienen la misma longitud.\n\n**Importante:** Para llevar a cabo un primer experimento, *dependiendo de la capacidad de procesamiento disponible para cada uno*, se sugiere ajustar el valor del parámetro **longitud de secuencia**, *hasta encontrar el valor más alto posible que, permita entrenar el modelo encoder-decoder, en un tiempo aceptable.*\n\nEl parámetro **longitud de secuencia** tiene un impacto importante en el entrenamiento del modelo. Un valor alto permite al modelo capturar más contexto en las frases, lo cual es crucial para traducir correctamente oraciones complejas; sin embargo, si la longitud es demasiado corta, el modelo puede truncar frases importantes, perdiendo información clave. Uso de memoria y eficiencia computacional:\n\nAdemás, longitudes mayores requieren más memoria, ya que el modelo debe manejar matrices más grandes para representar las secuencias. Mientras que, longitudes cortas son más eficientes en términos de recursos, pero pueden sacrificar precisión si las oraciones reales exceden ese límite con frecuencia.\n\nFinalmente, en traducción automática, las longitudes de las secuencias en el idioma origen y destino no siempre deben ser iguales (por ejemplo, una oración en inglés puede ser más corta que su equivalente en alemán).\n\nConsiderando lo anterior, si se dispone de infraestructura con GPU, se sugiere iniciar con un valor máximo de 12 (o cercano) y mínimo de 4. En la celda de código que sigue, se propone iniciar con el valor intermedio de 8, pero este valor puede ser ajustado.\n\nSi durante el entrenamiento, se presentan problemas (por limitación de infraestructura), se podría volver a este paso para fijar el valor mínimo de 4 para la longitud de secuencia de ambos idiomas, aunque, los resultados de la traducción serían pobres.\n\n\n**Salida esperada:** Tamaño de cada dataset y muestra de las tres primeras secuencias codificadas del dataset de entrenamiento.","metadata":{"id":"ZJbNLhqUc2ER"}},{"cell_type":"code","source":"# ----------------------\n# Dependiendo de la infraestructura del sistema,\n# la longitud de sencuencia se puede iniciar con un valor de 8.\nmax_text_length = 8\n# ----------------------\n\n# Representación numérica de las palabras (tokenización)\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ndef tokenization(sentences):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(sentences)\n    return tokenizer\n\n\n# Tokenización de las frases en inglés y en castellano\n#Tokenizar las frases en inglés\neng_tokenizer = tokenization(eng_spa[:, 0])\neng_vocab_size = len(eng_tokenizer.word_index) + 1\n\n#Tokenizar las frases en castellano\nspa_tokenizer = tokenization(eng_spa[:, 1])\nspa_vocab_size = len(spa_tokenizer.word_index) + 1\n\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndef encode_sequences(tokenizer, length, lines):\n    # Codificar las secuencias con los índices de las palabras\n    seq = tokenizer.texts_to_sequences(lines)\n    # Hacer el padding\n    seq = pad_sequences(seq, maxlen=length, padding='post')\n    return seq\n\ntrainX = encode_sequences(spa_tokenizer, max_text_length, train[:, 1])\ntrainY = encode_sequences(eng_tokenizer, max_text_length, train[:, 0])\n\ntestX = encode_sequences(spa_tokenizer, max_text_length, test[:, 1])\ntestY = encode_sequences(eng_tokenizer, max_text_length, test[:, 0])\n\n\nprint(trainX.shape, testX.shape)\ntrainX[:3]\n","metadata":{"id":"kNjdWyEdc2r4","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T06:01:57.650879Z","iopub.execute_input":"2025-06-13T06:01:57.651143Z","iopub.status.idle":"2025-06-13T06:02:01.630698Z","shell.execute_reply.started":"2025-06-13T06:01:57.651121Z","shell.execute_reply":"2025-06-13T06:02:01.630046Z"}},"outputs":[{"name":"stdout","text":"(54402, 8) (13601, 8)\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"array([[   6,  280,   35,  120,  882,    0,    0,    0],\n       [   4,   55,  522,    1,   31,  203, 2110,  201],\n       [  32,   48,  235,    1,    4, 5328,  237,  113]], dtype=int32)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"### 1.1.2 Definición del modelo encoder-decoder y entrenamiento (2 puntos)","metadata":{"id":"6M9ZQnwKqcAt"}},{"cell_type":"markdown","source":"**a. Definimos el modelo *encoder-decoder* basándonos en el notebook visto en la asignatura**, y lo instanciamos con una capa de embedding para las frases de la **lengua origen** y la dimensión de la última capa como el vocabulario de la **lengua destino**.\n\n**Importante:** Para la definición del modelo, considerar los siguientes parámetros y valores referenciales:\n\n* Como cantidad de **units** trabajar, inicialmente, con el valor de 100. El número de unidades o celdas de memoria en cada capa LSTM define la dimensionalidad del espacio interno en el que la LSTM procesa y representa la información a lo largo del tiempo; es decir, es el tamaño del vector de estado oculto *hidden state* y del estado de celda *cell state* que la LSTM mantiene para capturar patrones y dependencias en las secuencias de entrada.\n\n A mayor número de units, aumenta la capacidad del modelo para modelar relaciones complejas y dependencias a largo plazo en el texto, lo cual es clave en traducción automática donde el contexto puede abarcar varias palabras o frases. Sin embargo, un valor alto incrementa el número de parámetros, por tanto, se requerirá más memoria y tiempo de cómputo; además, crece el riesgo de sobreajuste si los datos de entrenamiento no son suficientes.\n\n Por lo indicado, trabajaremos con un valor inicial de 100, aunque, lo ideal sería usar un valor superior.\n\n* Longitud de los vectores de embeddings *embedding_vec_length* establecer en 200; este es un valor referencial que podría ser ajustado según el tamaño del vocabulario, la complejidad del idioma y los recursos disponibles. Más adelante, en el *ejercicio 1.1.3* se pedirá jugar un poco con este valor.\n\n**Resultado esperado:** se habrá instanciado un modelo encoder-decoder. Este modelo está diseñado para procesar y traducir textos del **idioma origen** al **idioma destino** utilizando capas de embedding y LSTM.\n\n*Salida esperada*: Utilizar el método *mt_model.summary()* para visualizar la estructura y configuración del modelo, incluyendo el número de parámetros y la disposición de las capas.","metadata":{"id":"9fYxSWIwoYSB"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\n\n# Definimos el modelo encoder-decoder\n\nembedding_vec_length = 200 # Valor referencial\nunits = 100 # Valor referencial\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding, RepeatVector\n\ndef define_model(in_vocab_size, embedding_vec_length, max_text_length, out_timesteps, out_vocab_size):\n    # Encoder\n    mt_model = Sequential()\n    mt_model.add(Embedding(in_vocab_size, embedding_vec_length, input_length = max_text_length, mask_zero=True))\n    mt_model.add(LSTM(units))\n\n    # Decoder\n    mt_model.add(RepeatVector(out_timesteps))\n    mt_model.add(LSTM(units, return_sequences=True))\n    mt_model.add(Dense(out_vocab_size, activation='softmax'))\n    \n    return mt_model\n\n#Definimos el modelo con una capa de embedding para las frases de la lengua origen (inglés)\nmt_model = define_model(eng_vocab_size, embedding_vec_length, max_text_length, max_text_length, spa_vocab_size)\n","metadata":{"id":"bGLbPLtE7nbN","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:51:06.138690Z","iopub.execute_input":"2025-06-13T07:51:06.138946Z","iopub.status.idle":"2025-06-13T07:51:06.155210Z","shell.execute_reply.started":"2025-06-13T07:51:06.138930Z","shell.execute_reply":"2025-06-13T07:51:06.154519Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**b. Compilamos el modelo**\n\n**Resultado esperado:** el modelo estará compilado y listo para ser entrenado. Se tiene que utilizar el optimizador *RMSprop* con una tasa de aprendizaje de *0.001* y la función de pérdida *sparse_categorical_crossentropy*.","metadata":{"id":"kuufosmqoxcA"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\n\n# Compilamos el modelo\nfrom keras import optimizers\n\nrms = optimizers.RMSprop(learning_rate=0.001)\nmt_model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\nmt_model.summary()","metadata":{"id":"wNwpJh4D7nbO","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:51:32.039772Z","iopub.execute_input":"2025-06-13T07:51:32.040134Z","iopub.status.idle":"2025-06-13T07:51:32.060513Z","shell.execute_reply.started":"2025-06-13T07:51:32.040087Z","shell.execute_reply":"2025-06-13T07:51:32.059989Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ repeat_vector_1 (\u001b[38;5;33mRepeatVector\u001b[0m)       │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ repeat_vector_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)       │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"**c. Entrenamos y guardamos el modelo.**\n\n**Importante:** El modelo puede tardar horas si se hace en CPU, y requerirá mucho menos si existen GPUs disponibles. Colab permite el uso de GPU en general, si no se hace un uso extensivo, y se va deshabilitando la opción y habilitando segun necesidades. Si se tiene activada siempre penaliza y la desactiva.\n\n* Por tanto, para probar si funciona, recomendamos lanzar el entrenamiento **solo con una época** y ver que funciona. Una vez tenemos claro que el flujo está funcionando, subimos el valor (por ejemplo, 50 o 100, dependiendo cómo evoluciona el modelo con cada *epoch*).\n\n* Si, durante el entrenamiento, Colab no puede cargar el modelo en memoria, recomendamos bajar el valor de **longitud de palabra** a 4 y el número de **units** a 128, de esta manera podríamos completar el proceso, aunque, seguramente, los resultados no serán buenos.\n\n* Revisar el `Notebook de Ejemplo`, en el que se proporcionan pautas y guías para llevar un mejor control de las ejecuciones cuando ocurren reinicios de sesión o saturación de memoria.","metadata":{"id":"IuVDwSApo0VT"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\n\n# Entrenamos y guardamos el modelo\n\nfrom keras.callbacks import ModelCheckpoint\n\nfilename = 'model_ta_eng_spa.keras'\n\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\nmt_model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n                    epochs=30, batch_size=256, validation_split = 0.2,callbacks=[checkpoint], \n                    verbose=1)","metadata":{"id":"_1muJyWc7nbO","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T07:51:51.578712Z","iopub.execute_input":"2025-06-13T07:51:51.578981Z","iopub.status.idle":"2025-06-13T08:02:42.282229Z","shell.execute_reply.started":"2025-06-13T07:51:51.578961Z","shell.execute_reply":"2025-06-13T08:02:42.281616Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 8.1732\nEpoch 1: val_loss improved from inf to 5.65771, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 121ms/step - loss: 8.1563 - val_loss: 5.6577\nEpoch 2/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.5883\nEpoch 2: val_loss improved from 5.65771 to 5.56077, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 5.5881 - val_loss: 5.5608\nEpoch 3/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 5.5146\nEpoch 3: val_loss improved from 5.56077 to 5.50866, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 133ms/step - loss: 5.5145 - val_loss: 5.5087\nEpoch 4/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 5.4621\nEpoch 4: val_loss improved from 5.50866 to 5.49027, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 5.4619 - val_loss: 5.4903\nEpoch 5/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 5.4166\nEpoch 5: val_loss improved from 5.49027 to 5.44383, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - loss: 5.4166 - val_loss: 5.4438\nEpoch 6/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.3636\nEpoch 6: val_loss improved from 5.44383 to 5.41499, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 5.3635 - val_loss: 5.4150\nEpoch 7/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 5.2993\nEpoch 7: val_loss improved from 5.41499 to 5.34256, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 128ms/step - loss: 5.2989 - val_loss: 5.3426\nEpoch 8/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 5.1893\nEpoch 8: val_loss improved from 5.34256 to 5.16766, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 5.1891 - val_loss: 5.1677\nEpoch 9/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.1120\nEpoch 9: val_loss improved from 5.16766 to 5.16665, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 5.1120 - val_loss: 5.1667\nEpoch 10/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.0641\nEpoch 10: val_loss did not improve from 5.16665\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 5.0641 - val_loss: 5.2942\nEpoch 11/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.0517\nEpoch 11: val_loss improved from 5.16665 to 5.08179, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 5.0515 - val_loss: 5.0818\nEpoch 12/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.0195\nEpoch 12: val_loss improved from 5.08179 to 5.04146, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 5.0193 - val_loss: 5.0415\nEpoch 13/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.0037\nEpoch 13: val_loss did not improve from 5.04146\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 5.0035 - val_loss: 5.2971\nEpoch 14/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9841\nEpoch 14: val_loss did not improve from 5.04146\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.9839 - val_loss: 5.1660\nEpoch 15/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9519\nEpoch 15: val_loss did not improve from 5.04146\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.9519 - val_loss: 5.0417\nEpoch 16/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9415\nEpoch 16: val_loss did not improve from 5.04146\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.9414 - val_loss: 5.3457\nEpoch 17/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9316\nEpoch 17: val_loss improved from 5.04146 to 5.00322, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.9313 - val_loss: 5.0032\nEpoch 18/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.8857\nEpoch 18: val_loss improved from 5.00322 to 4.92378, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.8857 - val_loss: 4.9238\nEpoch 19/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.8662\nEpoch 19: val_loss did not improve from 4.92378\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - loss: 4.8662 - val_loss: 4.9499\nEpoch 20/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.8444\nEpoch 20: val_loss improved from 4.92378 to 4.89834, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.8444 - val_loss: 4.8983\nEpoch 21/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.8254\nEpoch 21: val_loss did not improve from 4.89834\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.8254 - val_loss: 4.9364\nEpoch 22/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.8124\nEpoch 22: val_loss did not improve from 4.89834\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.8123 - val_loss: 4.9235\nEpoch 23/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.8008\nEpoch 23: val_loss improved from 4.89834 to 4.85311, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.8007 - val_loss: 4.8531\nEpoch 24/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.7694\nEpoch 24: val_loss did not improve from 4.85311\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.7694 - val_loss: 4.9764\nEpoch 25/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 4.7415\nEpoch 25: val_loss improved from 4.85311 to 4.84579, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 126ms/step - loss: 4.7416 - val_loss: 4.8458\nEpoch 26/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 4.7299\nEpoch 26: val_loss did not improve from 4.84579\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 124ms/step - loss: 4.7299 - val_loss: 4.9541\nEpoch 27/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 4.7240\nEpoch 27: val_loss did not improve from 4.84579\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.7240 - val_loss: 4.9689\nEpoch 28/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.7118\nEpoch 28: val_loss did not improve from 4.84579\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.7117 - val_loss: 4.9855\nEpoch 29/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.6791\nEpoch 29: val_loss improved from 4.84579 to 4.84018, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.6792 - val_loss: 4.8402\nEpoch 30/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.6737\nEpoch 30: val_loss did not improve from 4.84018\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.6737 - val_loss: 4.8575\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a4224b21290>"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"**d. Generar predicciones.**\n\nUna vez entrenado el modelo, aplicar el método *predict()* al conjunto de test para obtener las prediciones.\n\n**Sugerencia:**\n\n- Revisar el `Notebook de Ejemplo`, en el que proporciona una pauta para trabajar con un subconjunto del dataset de test, en caso de tener limitaciones durante el procesamiento.","metadata":{"id":"-Rnmfh3GpdJ0"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\n\n# Aplicamos el modelo\nimport os\nimport numpy as np\nfrom keras.models import load_model\n\nreducir_test = True   # poner True para quedarnos con solo las primeras 100 oraciones\nif reducir_test:\n    testX_to_pred = testX[:100]\nelse:\n    testX_to_pred = testX\n\nmodel = load_model(model_path, compile=False)\nprobs = model.predict(testX_to_pred, batch_size=64, verbose=1)\npreds = np.argmax(probs, axis=-1)\n\nprint(\"Preds shape:\", preds.shape)\nprint(preds[:2])","metadata":{"id":"QX74wIEu7nbP","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T08:06:11.971231Z","iopub.execute_input":"2025-06-13T08:06:11.971818Z","iopub.status.idle":"2025-06-13T08:06:12.979319Z","shell.execute_reply.started":"2025-06-13T08:06:11.971794Z","shell.execute_reply":"2025-06-13T08:06:12.978675Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 349ms/step\nPreds shape: (100, 8)\n[[7 6 1 1 0 0 0 0]\n [2 6 1 1 0 0 0 0]]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"**e. Visualización de resultados.**\n\nVisualizamos los resultados de las predicciones con los valores esperados.\n\n**Resultado esperado:** predicciones traducidas de las primeras 10 entradas del conjunto de prueba. Estas predicciones serán mostradas junto a los textos esperados para comparar.\n","metadata":{"id":"Drwik5f5pllA"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\n\n# Visualizamos resultados\ndef get_word(n, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == n:\n            return word\n    return None\n\npreds_text = []\n\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        #Obtener la palabra que corresponde al índice del vocabulario de la lengua destino\n        t = get_word(i[j], spa_tokenizer)\n        if j > 0:\n            if (t == get_word(i[j-1], spa_tokenizer)) or (t == None):\n                     temp.append('')\n            else:\n                     temp.append(t)\n        else:\n            if(t == None):\n                temp.append('')\n            else:\n                temp.append(t) \n\n    preds_text.append(' '.join(temp))\n\nimport pandas as pd\n\npred_df = pd.DataFrame({'actual' : test[:100,0], 'predicted' : preds_text})\n\npred_df.sample(10)","metadata":{"id":"5BNLHUKaLe9P","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T08:07:52.477910Z","iopub.execute_input":"2025-06-13T08:07:52.478198Z","iopub.status.idle":"2025-06-13T08:07:53.270803Z","shell.execute_reply.started":"2025-06-13T08:07:52.478178Z","shell.execute_reply":"2025-06-13T08:07:53.270147Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                                               actual         predicted\n30                     there's a hole in this bucket      de el de     \n69                            where is the city hall      en el de     \n6                                    tom is no angel     en el a de    \n7             everyone quits smoking sooner or later          de       \n46                        he almost never went there       la el a     \n93                    this is all you have to do now   la no que   no  \n38         many english words are derived from latin          de       \n73                          i don't believe in magic   en no que  de   \n82  have you ever seen a blind man swim  he probab...    la no que     \n98                       we started playing the game     la el a de    ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>actual</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30</th>\n      <td>there's a hole in this bucket</td>\n      <td>de el de</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>where is the city hall</td>\n      <td>en el de</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>tom is no angel</td>\n      <td>en el a de</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>everyone quits smoking sooner or later</td>\n      <td>de</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>he almost never went there</td>\n      <td>la el a</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>this is all you have to do now</td>\n      <td>la no que   no</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>many english words are derived from latin</td>\n      <td>de</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>i don't believe in magic</td>\n      <td>en no que  de</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>have you ever seen a blind man swim  he probab...</td>\n      <td>la no que</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>we started playing the game</td>\n      <td>la el a de</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"y_test = testY.reshape(testY.shape[0], testY.shape[1], 1)\n\nloss = mt_model.evaluate(\n    x=testX,         # shape (n_samples, max_text_length)\n    y=y_test,        # shape (n_samples, max_text_length, 1)\n    batch_size=256,\n    verbose=1\n)\n\nprint(\"Test loss:\", loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T08:13:02.311426Z","iopub.execute_input":"2025-06-13T08:13:02.312030Z","iopub.status.idle":"2025-06-13T08:13:05.192181Z","shell.execute_reply.started":"2025-06-13T08:13:02.312011Z","shell.execute_reply":"2025-06-13T08:13:05.191617Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 4.8832\nTest loss: 4.876737594604492\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"f. **Pregunta de análisis:** Según los resultados obtenidos en la predicción (valores reales vs. valores generados), ¿Por qué creéis que no son buenos, y como creéis que podrían obtenerse mejores resultados?","metadata":{"id":"UlScdExYLdqb"}},{"cell_type":"markdown","source":"**Respuesta a la pregunta**\n-\n\n\n\n\n\n","metadata":{"id":"VQGw9X9Rqsag"}},{"cell_type":"markdown","source":"Porque tanto loss como val_loss que obtenemos son muy altos. El modelo solo llega a \"aprender\" las palabras más habituales. Si las pérdidas no fuesen tan altas podríamos aumentar units, entrenar el modelo durante más épocas, modificar LR, longitud de secuencia, de embeddings, etc.\nPero en este caso, tendría más impacto crear un modelo más completo, usando capas LSTM más grande, añadiendo más capas y mecanismos de atención o directamente transformers.","metadata":{}},{"cell_type":"markdown","source":"### 1.1.3 Experimentación con diferentes resultados (1,5 puntos)","metadata":{"id":"mqE2RI7dqcAw"}},{"cell_type":"markdown","source":"\nEn este apartado, podríamos analizar cómo afecta a la calidad de la traducción, la variación de distintos parámetros del modelo, como longitud de embeddings (*embedding_vec_length*), longitud de secuencia (*max_text_length*), número de units (*units*), batch size, epochs etc. Sin embargo, debido a que no siempre encontraremos GPUs libres, aquí vamos vamos a limitarnos a experimentar con los parámetros ** *embedding_vec_length* y *max_text_length*.\n\n**Importante:** durante las ejecuciones, dependiendo del modelo y del consumo de memoria actual, la predicción se puede cancelar por agotar toda la memoria disponible. En caso de cancelación, recargar el modelo desde local (ver apartado 1.1.2.2 del `Notebook de Ejemplo`) y predecir solo para un subconjunto del fichero de test (aunque en este caso no serán válidas las magnitudes de medición de calidad del sistema).  \n\n\nAdemás, se sugiere que tras cada entrenamiento, se realice una copia del modelo entrenado y se almacene en local *'model/..'* para, en caso de cancelación, no tener que realizar de nuevo el correspondiente entrenamiento, asociando a la copia los parámetros con los que se entrenó.\n","metadata":{"id":"-KKCg5OtiF7w"}},{"cell_type":"markdown","source":"**a. Experimentar con el valor de longitud de embedding** (*embedding_vec_length*)\n\nAnalizar cómo un incremento/reducción en el tamaño de los vectores de embedding afecta el rendimiento de un modelo de traducción automática del **idioma origen** al **idioma destino**.\n\n**Resultado esperado:** Se imprimirá el resultado que muestre el rendimiento del modelo creado para diferentes tamaños de embedding (inicialmente, se sugirió trabajar con 200; ahora, se podría experimentar con valores como 50 y 300). Cada resultado constará del tamaño del embedding seguido de un *score*, que indique la efectividad del modelo calculado con *model.evaluate()*\n","metadata":{"id":"hT7AWfrvTFFo"}},{"cell_type":"code","source":"# Posibles tamaños de vectores a probar:\nembedding_sizes = [50, 300]\nepochs = 30 # incrementar este valor si existe disponibilidad de GPUs.\nbatch_size=128\n\n#############################################\n# SOLUCIÓN                                  #\n#############################################\n\nembedding_vec_length = 50\n\ndef define_model(in_vocab_size, embedding_vec_length, max_text_length, out_timesteps, out_vocab_size):\n    # Encoder\n    mt_model = Sequential()\n    mt_model.add(Embedding(in_vocab_size, embedding_vec_length, input_length = max_text_length, mask_zero=True))\n    mt_model.add(LSTM(units))\n\n    # Decoder\n    mt_model.add(RepeatVector(out_timesteps))\n    mt_model.add(LSTM(units, return_sequences=True))\n    mt_model.add(Dense(out_vocab_size, activation='softmax'))\n    \n    return mt_model\n\n#Definimos el modelo con una capa de embedding para las frases de la lengua origen (inglés)\nmt_model = define_model(eng_vocab_size, embedding_vec_length, max_text_length, max_text_length, spa_vocab_size)\n\n# Compilar\nrms = optimizers.RMSprop(learning_rate=0.001)\nmt_model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n\n# Entrenar\nfilename = 'model_ta_eng_spa.keras'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nmt_model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n                    epochs=30, batch_size=256, validation_split = 0.2,callbacks=[checkpoint], \n                    verbose=1)\n\n","metadata":{"id":"V_OIoFheEKfu","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T08:13:54.761044Z","iopub.execute_input":"2025-06-13T08:13:54.761744Z","iopub.status.idle":"2025-06-13T08:24:42.902400Z","shell.execute_reply.started":"2025-06-13T08:13:54.761719Z","shell.execute_reply":"2025-06-13T08:24:42.901679Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ repeat_vector_2 (\u001b[38;5;33mRepeatVector\u001b[0m)       │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ repeat_vector_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)       │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 8.1687\nEpoch 1: val_loss improved from inf to 5.64790, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 124ms/step - loss: 8.1519 - val_loss: 5.6479\nEpoch 2/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 5.5930\nEpoch 2: val_loss improved from 5.64790 to 5.59436, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 133ms/step - loss: 5.5927 - val_loss: 5.5944\nEpoch 3/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 5.5008\nEpoch 3: val_loss improved from 5.59436 to 5.50498, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 129ms/step - loss: 5.5007 - val_loss: 5.5050\nEpoch 4/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: 5.4576\nEpoch 4: val_loss improved from 5.50498 to 5.47147, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 123ms/step - loss: 5.4575 - val_loss: 5.4715\nEpoch 5/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 5.4227\nEpoch 5: val_loss did not improve from 5.47147\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 122ms/step - loss: 5.4226 - val_loss: 5.6143\nEpoch 6/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 5.4005\nEpoch 6: val_loss improved from 5.47147 to 5.39291, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 5.4003 - val_loss: 5.3929\nEpoch 7/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.3305\nEpoch 7: val_loss improved from 5.39291 to 5.36078, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 5.3302 - val_loss: 5.3608\nEpoch 8/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 5.2179\nEpoch 8: val_loss improved from 5.36078 to 5.31986, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 5.2176 - val_loss: 5.3199\nEpoch 9/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 5.1284\nEpoch 9: val_loss did not improve from 5.31986\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 123ms/step - loss: 5.1282 - val_loss: 5.4800\nEpoch 10/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 5.0802\nEpoch 10: val_loss improved from 5.31986 to 5.09706, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 5.0801 - val_loss: 5.0971\nEpoch 11/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 5.0338\nEpoch 11: val_loss did not improve from 5.09706\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 5.0338 - val_loss: 5.1675\nEpoch 12/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.0161\nEpoch 12: val_loss did not improve from 5.09706\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 5.0160 - val_loss: 5.2075\nEpoch 13/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9849\nEpoch 13: val_loss did not improve from 5.09706\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.9849 - val_loss: 5.2159\nEpoch 14/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9802\nEpoch 14: val_loss improved from 5.09706 to 5.00944, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.9801 - val_loss: 5.0094\nEpoch 15/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9520\nEpoch 15: val_loss did not improve from 5.00944\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.9520 - val_loss: 5.2070\nEpoch 16/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9387\nEpoch 16: val_loss improved from 5.00944 to 5.00131, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.9388 - val_loss: 5.0013\nEpoch 17/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9308\nEpoch 17: val_loss did not improve from 5.00131\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.9308 - val_loss: 5.0521\nEpoch 18/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9144\nEpoch 18: val_loss did not improve from 5.00131\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.9143 - val_loss: 5.0031\nEpoch 19/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.8968\nEpoch 19: val_loss did not improve from 5.00131\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.8968 - val_loss: 5.1527\nEpoch 20/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.8891\nEpoch 20: val_loss improved from 5.00131 to 4.93557, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.8890 - val_loss: 4.9356\nEpoch 21/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.8640\nEpoch 21: val_loss improved from 4.93557 to 4.92934, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.8640 - val_loss: 4.9293\nEpoch 22/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.8562\nEpoch 22: val_loss did not improve from 4.92934\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.8561 - val_loss: 5.2025\nEpoch 23/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 4.8477\nEpoch 23: val_loss improved from 4.92934 to 4.88417, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.8474 - val_loss: 4.8842\nEpoch 24/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.8112\nEpoch 24: val_loss improved from 4.88417 to 4.86090, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.8111 - val_loss: 4.8609\nEpoch 25/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.7860\nEpoch 25: val_loss did not improve from 4.86090\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.7860 - val_loss: 4.9038\nEpoch 26/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.7545\nEpoch 26: val_loss improved from 4.86090 to 4.83032, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.7546 - val_loss: 4.8303\nEpoch 27/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.7472\nEpoch 27: val_loss did not improve from 4.83032\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 4.7471 - val_loss: 4.8692\nEpoch 28/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.7248\nEpoch 28: val_loss did not improve from 4.83032\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.7247 - val_loss: 4.9699\nEpoch 29/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.7135\nEpoch 29: val_loss improved from 4.83032 to 4.81699, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.7134 - val_loss: 4.8170\nEpoch 30/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.6862\nEpoch 30: val_loss improved from 4.81699 to 4.78108, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.6862 - val_loss: 4.7811\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a41d8555210>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"y_test = testY.reshape(testY.shape[0], testY.shape[1], 1)\n\n# 2) Llama a evaluate sobre X e Y:\nloss = mt_model.evaluate(\n    x=testX,         # shape (n_samples, max_text_length)\n    y=y_test,        # shape (n_samples, max_text_length, 1)\n    batch_size=256,\n    verbose=1\n)\n\nprint(\"Tamaño del embedding: 50\")\nprint(\"Test loss:\", loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T08:25:40.502718Z","iopub.execute_input":"2025-06-13T08:25:40.503331Z","iopub.status.idle":"2025-06-13T08:25:42.889639Z","shell.execute_reply.started":"2025-06-13T08:25:40.503309Z","shell.execute_reply":"2025-06-13T08:25:42.889058Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 4.8027\nTamaño del embedding: 50\nTest loss: 4.793683052062988\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"Longitud de embedding: 300","metadata":{}},{"cell_type":"code","source":"embedding_vec_length = 300\n\ndef define_model(in_vocab_size, embedding_vec_length, max_text_length, out_timesteps, out_vocab_size):\n    # Encoder\n    mt_model = Sequential()\n    mt_model.add(Embedding(in_vocab_size, embedding_vec_length, input_length = max_text_length, mask_zero=True))\n    mt_model.add(LSTM(units))\n\n    # Decoder\n    mt_model.add(RepeatVector(out_timesteps))\n    mt_model.add(LSTM(units, return_sequences=True))\n    mt_model.add(Dense(out_vocab_size, activation='softmax'))\n    \n    return mt_model\n\n#Definimos el modelo con una capa de embedding para las frases de la lengua origen (inglés)\nmt_model = define_model(eng_vocab_size, embedding_vec_length, max_text_length, max_text_length, spa_vocab_size)\n\n# Compilar\nrms = optimizers.RMSprop(learning_rate=0.001)\nmt_model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n\n# Entrenar\nfilename = 'model_ta_eng_spa.keras'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nmt_model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n                    epochs=30, batch_size=256, validation_split = 0.2,callbacks=[checkpoint], \n                    verbose=1)\n\ny_test = testY.reshape(testY.shape[0], testY.shape[1], 1)\n\nloss = mt_model.evaluate(\n    x=testX,\n    y=y_test,\n    batch_size=256,\n    verbose=1\n)\n\nprint(\"Tamaño del embedding: 300\")\nprint(\"Test loss:\", loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T08:25:53.009881Z","iopub.execute_input":"2025-06-13T08:25:53.010654Z","iopub.status.idle":"2025-06-13T08:36:53.582542Z","shell.execute_reply.started":"2025-06-13T08:25:53.010629Z","shell.execute_reply":"2025-06-13T08:36:53.581817Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 8.1532\nEpoch 1: val_loss improved from inf to 5.64166, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 134ms/step - loss: 8.1364 - val_loss: 5.6417\nEpoch 2/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 5.5978\nEpoch 2: val_loss improved from 5.64166 to 5.55828, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 134ms/step - loss: 5.5975 - val_loss: 5.5583\nEpoch 3/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 5.5206\nEpoch 3: val_loss improved from 5.55828 to 5.52982, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 5.5204 - val_loss: 5.5298\nEpoch 4/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 5.4690\nEpoch 4: val_loss improved from 5.52982 to 5.49657, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 5.4689 - val_loss: 5.4966\nEpoch 5/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 5.4279\nEpoch 5: val_loss improved from 5.49657 to 5.45007, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 129ms/step - loss: 5.4278 - val_loss: 5.4501\nEpoch 6/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 5.3871\nEpoch 6: val_loss improved from 5.45007 to 5.43452, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 130ms/step - loss: 5.3871 - val_loss: 5.4345\nEpoch 7/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 5.3365\nEpoch 7: val_loss improved from 5.43452 to 5.27061, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 128ms/step - loss: 5.3361 - val_loss: 5.2706\nEpoch 8/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.1974\nEpoch 8: val_loss did not improve from 5.27061\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 5.1972 - val_loss: 5.2836\nEpoch 9/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.1216\nEpoch 9: val_loss improved from 5.27061 to 5.11131, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 5.1213 - val_loss: 5.1113\nEpoch 10/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.0601\nEpoch 10: val_loss improved from 5.11131 to 5.07266, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 5.0600 - val_loss: 5.0727\nEpoch 11/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.0228\nEpoch 11: val_loss did not improve from 5.07266\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 5.0227 - val_loss: 5.0787\nEpoch 12/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 5.0014\nEpoch 12: val_loss did not improve from 5.07266\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - loss: 5.0013 - val_loss: 5.1869\nEpoch 13/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9764\nEpoch 13: val_loss improved from 5.07266 to 5.00127, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.9763 - val_loss: 5.0013\nEpoch 14/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9570\nEpoch 14: val_loss improved from 5.00127 to 4.98731, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.9569 - val_loss: 4.9873\nEpoch 15/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 4.9337\nEpoch 15: val_loss did not improve from 4.98731\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - loss: 4.9337 - val_loss: 5.1070\nEpoch 16/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.9112\nEpoch 16: val_loss improved from 4.98731 to 4.97957, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 128ms/step - loss: 4.9111 - val_loss: 4.9796\nEpoch 17/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.8980\nEpoch 17: val_loss did not improve from 4.97957\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.8979 - val_loss: 5.0080\nEpoch 18/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 4.8666\nEpoch 18: val_loss did not improve from 4.97957\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.8666 - val_loss: 5.2486\nEpoch 19/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 4.8517\nEpoch 19: val_loss improved from 4.97957 to 4.91826, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 129ms/step - loss: 4.8516 - val_loss: 4.9183\nEpoch 20/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 4.8357\nEpoch 20: val_loss did not improve from 4.91826\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.8356 - val_loss: 4.9322\nEpoch 21/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.8064\nEpoch 21: val_loss did not improve from 4.91826\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.8065 - val_loss: 4.9585\nEpoch 22/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.7942\nEpoch 22: val_loss did not improve from 4.91826\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.7943 - val_loss: 5.0569\nEpoch 23/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.8007\nEpoch 23: val_loss did not improve from 4.91826\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.8005 - val_loss: 4.9772\nEpoch 24/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.7750\nEpoch 24: val_loss improved from 4.91826 to 4.90764, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 128ms/step - loss: 4.7749 - val_loss: 4.9076\nEpoch 25/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.7544\nEpoch 25: val_loss improved from 4.90764 to 4.83371, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 129ms/step - loss: 4.7544 - val_loss: 4.8337\nEpoch 26/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.7300\nEpoch 26: val_loss did not improve from 4.83371\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.7301 - val_loss: 4.9984\nEpoch 27/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 4.7261\nEpoch 27: val_loss did not improve from 4.83371\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 128ms/step - loss: 4.7261 - val_loss: 4.8863\nEpoch 28/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 4.7040\nEpoch 28: val_loss did not improve from 4.83371\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.7040 - val_loss: 5.0131\nEpoch 29/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.6827\nEpoch 29: val_loss did not improve from 4.83371\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.6828 - val_loss: 4.9105\nEpoch 30/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 4.6804\nEpoch 30: val_loss did not improve from 4.83371\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - loss: 4.6802 - val_loss: 4.8375\n\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step - loss: 4.8604\nTamaño del embedding: 300\nTest loss: 4.853137016296387\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"**b. Ejercicio opcional: Experimentar con el valor longitud de secuencia** (*max_text_length*).\n\nAnalizar cómo un incremento/reducción de la longitud de secuencia impacta a la calidad del modelo de traducción.\n\n**Resultado esperado:** Se imprimirá el resultado que muestre el rendimiento del modelo para una longitud de secuencia superior o inferior al establecido (por ejemplo, si se inicializó el modelo preliminar con el valor de 8, aquí se podría probar con 4 y 12). El resultado constará del valor de la longitud, y del score que indica la efectividad del modelo calculado con *model.evaluate()*","metadata":{"id":"fdUrxdHXTIe5"}},{"cell_type":"markdown","source":"max_text_length = 4","metadata":{}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\nmax_text_length = 4\n\nembedding_vec_length = 200\n\n\n#Definimos el modelo con una capa de embedding para las frases de la lengua origen (inglés)\nmt_model = define_model(eng_vocab_size, embedding_vec_length, max_text_length, max_text_length, spa_vocab_size)\n\n# Compilar\nrms = optimizers.RMSprop(learning_rate=0.001)\nmt_model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n\ntrainY_cut = trainY[:, :max_text_length]\ntestY_cut  = testY[:,  :max_text_length]\n\ny_train = trainY_cut.reshape(-1, max_text_length, 1)\ny_test  = testY_cut.reshape(-1,  max_text_length, 1)\n\n# Entrenar\nfilename = 'model_ta_eng_spa.keras'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nmt_model.fit(trainX, y_train,\n                    epochs=30, batch_size=256, validation_split = 0.2,callbacks=[checkpoint], \n                    verbose=1)\n\n# y_test = testY.reshape(testY.shape[0], testY.shape[1], 1)\n\nloss = mt_model.evaluate(\n    x=testX,         # shape (n_samples, max_text_length)\n    y=y_test,        # shape (n_samples, max_text_length, 1)\n    batch_size=256,\n    verbose=1\n)\n\nprint(\"Tamaño del embedding: 200\")\nprint(\"max_text_length: 4\")\nprint(\"Test loss:\", loss)","metadata":{"id":"qPgcB87FT-61","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T08:49:19.889187Z","iopub.execute_input":"2025-06-13T08:49:19.889881Z","iopub.status.idle":"2025-06-13T08:56:51.496080Z","shell.execute_reply.started":"2025-06-13T08:49:19.889856Z","shell.execute_reply":"2025-06-13T08:56:51.495529Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 9.2716\nEpoch 1: val_loss improved from inf to 6.59261, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 87ms/step - loss: 9.2569 - val_loss: 6.5926\nEpoch 2/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 6.5390\nEpoch 2: val_loss improved from 6.59261 to 6.47158, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - loss: 6.5386 - val_loss: 6.4716\nEpoch 3/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.4202\nEpoch 3: val_loss improved from 6.47158 to 6.38748, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.4200 - val_loss: 6.3875\nEpoch 4/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.3421\nEpoch 4: val_loss improved from 6.38748 to 6.34228, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.3420 - val_loss: 6.3423\nEpoch 5/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 6.2857\nEpoch 5: val_loss improved from 6.34228 to 6.29344, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 87ms/step - loss: 6.2855 - val_loss: 6.2934\nEpoch 6/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 6.2418\nEpoch 6: val_loss improved from 6.29344 to 6.26599, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 84ms/step - loss: 6.2417 - val_loss: 6.2660\nEpoch 7/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 6.2135\nEpoch 7: val_loss improved from 6.26599 to 6.24693, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 85ms/step - loss: 6.2134 - val_loss: 6.2469\nEpoch 8/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.1900\nEpoch 8: val_loss improved from 6.24693 to 6.22928, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.1900 - val_loss: 6.2293\nEpoch 9/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.1749\nEpoch 9: val_loss improved from 6.22928 to 6.20816, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.1748 - val_loss: 6.2082\nEpoch 10/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.1414\nEpoch 10: val_loss improved from 6.20816 to 6.19122, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.1415 - val_loss: 6.1912\nEpoch 11/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.1292\nEpoch 11: val_loss improved from 6.19122 to 6.17612, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.1292 - val_loss: 6.1761\nEpoch 12/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.1119\nEpoch 12: val_loss improved from 6.17612 to 6.16692, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.1119 - val_loss: 6.1669\nEpoch 13/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.0905\nEpoch 13: val_loss improved from 6.16692 to 6.15064, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.0906 - val_loss: 6.1506\nEpoch 14/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.0878\nEpoch 14: val_loss improved from 6.15064 to 6.14292, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.0878 - val_loss: 6.1429\nEpoch 15/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.0832\nEpoch 15: val_loss did not improve from 6.14292\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - loss: 6.0831 - val_loss: 6.1454\nEpoch 16/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.0554\nEpoch 16: val_loss improved from 6.14292 to 6.12862, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.0555 - val_loss: 6.1286\nEpoch 17/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.0603\nEpoch 17: val_loss improved from 6.12862 to 6.12847, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.0603 - val_loss: 6.1285\nEpoch 18/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.0421\nEpoch 18: val_loss improved from 6.12847 to 6.12476, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.0421 - val_loss: 6.1248\nEpoch 19/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.0308\nEpoch 19: val_loss improved from 6.12476 to 6.09594, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.0309 - val_loss: 6.0959\nEpoch 20/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.0295\nEpoch 20: val_loss did not improve from 6.09594\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - loss: 6.0295 - val_loss: 6.1240\nEpoch 21/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.0038\nEpoch 21: val_loss did not improve from 6.09594\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - loss: 6.0039 - val_loss: 6.1080\nEpoch 22/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 6.0022\nEpoch 22: val_loss improved from 6.09594 to 6.08194, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 6.0021 - val_loss: 6.0819\nEpoch 23/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.9799\nEpoch 23: val_loss improved from 6.08194 to 6.05411, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 5.9799 - val_loss: 6.0541\nEpoch 24/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.9578\nEpoch 24: val_loss improved from 6.05411 to 6.02208, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 5.9578 - val_loss: 6.0221\nEpoch 25/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.9207\nEpoch 25: val_loss improved from 6.02208 to 6.00508, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 5.9207 - val_loss: 6.0051\nEpoch 26/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.8827\nEpoch 26: val_loss improved from 6.00508 to 5.95366, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 5.8828 - val_loss: 5.9537\nEpoch 27/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.8547\nEpoch 27: val_loss improved from 5.95366 to 5.95106, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 5.8547 - val_loss: 5.9511\nEpoch 28/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.8242\nEpoch 28: val_loss did not improve from 5.95106\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - loss: 5.8241 - val_loss: 6.0272\nEpoch 29/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.7925\nEpoch 29: val_loss improved from 5.95106 to 5.86868, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 88ms/step - loss: 5.7924 - val_loss: 5.8687\nEpoch 30/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 5.7504\nEpoch 30: val_loss did not improve from 5.86868\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 86ms/step - loss: 5.7504 - val_loss: 5.8815\n\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 5.9080\nTamaño del embedding: 300\nTest loss: 5.909079074859619\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"max_text_length = 12","metadata":{}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\nmax_text_length = 12\n\nembedding_vec_length = 200\n\ntrainX = pad_sequences(trainX, maxlen=max_text_length, padding='post')\ntrainY = pad_sequences(trainY, maxlen=max_text_length, padding='post')\ntestX  = pad_sequences(testX,  maxlen=max_text_length, padding='post')\ntestY  = pad_sequences(testY,  maxlen=max_text_length, padding='post')\n\n#Definimos el modelo con una capa de embedding para las frases de la lengua origen (inglés)\nmt_model = define_model(eng_vocab_size, embedding_vec_length, max_text_length, max_text_length, spa_vocab_size)\n\n# Compilar\nrms = optimizers.RMSprop(learning_rate=0.001)\nmt_model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n\ntrainY_cut = trainY[:, :max_text_length]\ntestY_cut  = testY[:,  :max_text_length]\n\ny_train = trainY_cut.reshape(-1, max_text_length, 1)\ny_test  = testY_cut.reshape(-1,  max_text_length, 1)\n\n# Entrenar\nfilename = 'model_ta_eng_spa.keras'\ncheckpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nmt_model.fit(trainX, y_train,\n                    epochs=30, batch_size=256, validation_split = 0.2,callbacks=[checkpoint], \n                    verbose=1)\n\n# y_test = testY.reshape(testY.shape[0], testY.shape[1], 1)\n\nloss = mt_model.evaluate(\n    x=testX,         # shape (n_samples, max_text_length)\n    y=y_test,        # shape (n_samples, max_text_length, 1)\n    batch_size=256,\n    verbose=1\n)\n\nprint(\"Tamaño del embedding: 200\")\nprint(\"max_text_length: 12\")\nprint(\"Test loss:\", loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T09:14:00.218189Z","iopub.execute_input":"2025-06-13T09:14:00.218869Z","iopub.status.idle":"2025-06-13T09:28:24.656694Z","shell.execute_reply.started":"2025-06-13T09:14:00.218847Z","shell.execute_reply":"2025-06-13T09:28:24.656140Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 7.0449\nEpoch 1: val_loss improved from inf to 4.05409, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 156ms/step - loss: 7.0243 - val_loss: 4.0541\nEpoch 2/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - loss: 4.0175\nEpoch 2: val_loss improved from 4.05409 to 3.99299, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 174ms/step - loss: 4.0171 - val_loss: 3.9930\nEpoch 3/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 3.8816\nEpoch 3: val_loss improved from 3.99299 to 3.72551, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 169ms/step - loss: 3.8810 - val_loss: 3.7255\nEpoch 4/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 3.6635\nEpoch 4: val_loss did not improve from 3.72551\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 163ms/step - loss: 3.6631 - val_loss: 3.7687\nEpoch 5/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 3.5506\nEpoch 5: val_loss improved from 3.72551 to 3.62149, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 171ms/step - loss: 3.5502 - val_loss: 3.6215\nEpoch 6/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 3.4748\nEpoch 6: val_loss improved from 3.62149 to 3.49169, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 168ms/step - loss: 3.4747 - val_loss: 3.4917\nEpoch 7/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 3.4355\nEpoch 7: val_loss did not improve from 3.49169\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 164ms/step - loss: 3.4355 - val_loss: 3.6721\nEpoch 8/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 3.4133\nEpoch 8: val_loss improved from 3.49169 to 3.41093, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 165ms/step - loss: 3.4133 - val_loss: 3.4109\nEpoch 9/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 3.3866\nEpoch 9: val_loss did not improve from 3.41093\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 165ms/step - loss: 3.3866 - val_loss: 3.4741\nEpoch 10/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 3.3834\nEpoch 10: val_loss improved from 3.41093 to 3.38957, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 167ms/step - loss: 3.3833 - val_loss: 3.3896\nEpoch 11/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 3.3626\nEpoch 11: val_loss did not improve from 3.38957\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 166ms/step - loss: 3.3626 - val_loss: 3.5587\nEpoch 12/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 3.3614\nEpoch 12: val_loss improved from 3.38957 to 3.38612, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 169ms/step - loss: 3.3613 - val_loss: 3.3861\nEpoch 13/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 3.3400\nEpoch 13: val_loss improved from 3.38612 to 3.36232, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 169ms/step - loss: 3.3401 - val_loss: 3.3623\nEpoch 14/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 3.3406\nEpoch 14: val_loss did not improve from 3.36232\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 167ms/step - loss: 3.3406 - val_loss: 3.3648\nEpoch 15/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 3.3414\nEpoch 15: val_loss did not improve from 3.36232\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 168ms/step - loss: 3.3413 - val_loss: 3.3808\nEpoch 16/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - loss: 3.3177\nEpoch 16: val_loss did not improve from 3.36232\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 170ms/step - loss: 3.3178 - val_loss: 3.5311\nEpoch 17/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 3.3339\nEpoch 17: val_loss did not improve from 3.36232\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 168ms/step - loss: 3.3337 - val_loss: 3.5373\nEpoch 18/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 3.3147\nEpoch 18: val_loss improved from 3.36232 to 3.33285, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 169ms/step - loss: 3.3147 - val_loss: 3.3328\nEpoch 19/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 3.3162\nEpoch 19: val_loss did not improve from 3.33285\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 167ms/step - loss: 3.3161 - val_loss: 3.3994\nEpoch 20/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 3.2924\nEpoch 20: val_loss did not improve from 3.33285\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 167ms/step - loss: 3.2925 - val_loss: 3.3619\nEpoch 21/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 3.2903\nEpoch 21: val_loss improved from 3.33285 to 3.33137, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 168ms/step - loss: 3.2904 - val_loss: 3.3314\nEpoch 22/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 3.2940\nEpoch 22: val_loss did not improve from 3.33137\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 166ms/step - loss: 3.2939 - val_loss: 3.5210\nEpoch 23/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 3.2901\nEpoch 23: val_loss improved from 3.33137 to 3.32914, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 169ms/step - loss: 3.2900 - val_loss: 3.3291\nEpoch 24/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 3.2766\nEpoch 24: val_loss did not improve from 3.32914\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 166ms/step - loss: 3.2766 - val_loss: 3.4369\nEpoch 25/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 3.2763\nEpoch 25: val_loss did not improve from 3.32914\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 167ms/step - loss: 3.2763 - val_loss: 3.3483\nEpoch 26/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 3.2648\nEpoch 26: val_loss did not improve from 3.32914\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 168ms/step - loss: 3.2649 - val_loss: 3.5225\nEpoch 27/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 3.2706\nEpoch 27: val_loss improved from 3.32914 to 3.31981, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 169ms/step - loss: 3.2705 - val_loss: 3.3198\nEpoch 28/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 3.2594\nEpoch 28: val_loss did not improve from 3.31981\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 168ms/step - loss: 3.2594 - val_loss: 3.4523\nEpoch 29/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 3.2496\nEpoch 29: val_loss did not improve from 3.31981\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 168ms/step - loss: 3.2497 - val_loss: 3.3215\nEpoch 30/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - loss: 3.2485\nEpoch 30: val_loss improved from 3.31981 to 3.30239, saving model to model_ta_eng_spa.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 170ms/step - loss: 3.2485 - val_loss: 3.3024\n\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 50ms/step - loss: 3.3162\nTamaño del embedding: 300\nTest loss: 3.310957193374634\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"Según los resultados obtenidos en este ejercicio 1.1.3, discutir en el *documento de Análisis* las diferencias encontradas.","metadata":{"id":"Cyb8LvH0rqrq"}},{"cell_type":"markdown","source":"## 1.2 TA con Embeddings preentrenados (2,5 puntos)","metadata":{"id":"Re2z6jLm7nbP"}},{"cell_type":"markdown","source":"En este apartado repetiremos el ejercicio anterior cargando a la capa de embedding los pesos d'un modelo GloVe entrenado para el inglés.\n\nEste apartado 1.2 puede ejecutarse en diferentes sesiones de trabajo y no depende de las secciones anteriores a excepción de:\n* Ejecutar el apartado *0. Conexión con Drive* (o las celdas que se hayan definido para otros entornos no Colab).\n* Ejecutar las celda de preparación de datos (sección 1.1.1)\n* Ejecutar las celdas, cuando sea necesario, del apartado *1.1.2.2 Cargar desde checkpoint y continuar entrenamiento* e invocar las correspondientes funciones con el path del modelo guardado en local correspondiente a este apartado sobre Embeddings preentrenados.","metadata":{"id":"_zzbdDJq7nbP"}},{"cell_type":"markdown","source":"### 1.2.1 Carga de GloVe","metadata":{"id":"P3MS7zPlqcAy"}},{"cell_type":"markdown","source":"**a. Empezamos cargando el modelo GloVe para el inglés.**\n\nPodéis usar 'glove.42B.300d.txt'.(https://www.kaggle.com/datasets/yutanakamura/glove42b300dtxt)\n\n**Salida esperada:** tamaño del objeto cargado, usar *len()*.","metadata":{"id":"9vncpo5D7nbP"}},{"cell_type":"code","source":"import numpy as np\n\nembeddings_index = {}\n\nglove=\"glove.42B.300d.txt\"\nglove_path = f\"/kaggle/input/glove42b300dtxt/glove.42B.300d.txt\"    #'my_path_pra2' definida en sección 0\n\n\nprint(f\"Attempting to open glove file: {glove_path}\")\ntry:\n    with open(glove_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n        print(f\"Successfully opened glove file: {glove}\")\nexcept Exception as e:\n    print(f\"ERROR: File not found at opening file: {e}\")\n\nfile_path = f\"/kaggle/input/data-eng-spa/data_eng_spa.tsv\"\neng_spa = pd.read_csv(file_path, sep=\"\\t\", header=None, usecols=[1, 3])\n\nprint(len(embeddings_index))\n","metadata":{"id":"CKCaeep9B29Z","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T11:51:18.908972Z","iopub.status.idle":"2025-06-14T11:51:18.909331Z","shell.execute_reply.started":"2025-06-14T11:51:18.909141Z","shell.execute_reply":"2025-06-14T11:51:18.909159Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2.2 Definición del modelo","metadata":{"id":"bCu6o3oubiAI"}},{"cell_type":"markdown","source":"**a. Construir la matriz de embeddings.**\n  \nA continuación, tenemos que construir la matriz de embeddings. Para no cargar todo el vocabulario del modelo, podemos filtrar solo aquellas entradas presentes en el vocabulario del tokenizador que usaremos. Además, debemos de incluir en la matriz de vectores correspondientes los índices de las entradas (palabras) que no encontremos en el modelo glove cargado. Estos vectores se suelen inicializar con 0s o con el resultado de una distribución N (0,1).\n\n**Salida esperada:** imprimir los 3 primeros elementos de la matriz de embeddings.","metadata":{"id":"kajSHKSH7nbR"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\nembedding_vec_length = 200\neng_sentences = eng_spa.iloc[:, 0].values\neng_tokenizer = tokenization(eng_sentences)\neng_vocab_size = len(eng_tokenizer.word_index) + 1\n\nembedding_matrix = np.zeros((eng_vocab_size, embedding_vec_length))\nfor word, idx in eng_tokenizer.word_index.items():\n    vector = embeddings_index.get(word)\n    if vector is not None:\n        embedding_matrix[idx] = vector\n    else:\n        # OOV: inicializa con valores de N(0,1)\n        embedding_matrix[idx] = np.random.normal(size=(embedding_vec_length,))\n\nprint(\"Primeras 3 filas de la matriz de embeddings:\")\nprint(embedding_matrix[:3])","metadata":{"id":"T0-OCm4XqcA4","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T11:57:45.815251Z","iopub.execute_input":"2025-06-14T11:57:45.815544Z","iopub.status.idle":"2025-06-14T11:57:45.825414Z","shell.execute_reply.started":"2025-06-14T11:57:45.815524Z","shell.execute_reply":"2025-06-14T11:57:45.824484Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3825167009.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#############################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0membedding_vec_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0meng_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meng_spa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0meng_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0meng_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meng_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'eng_spa' is not defined"],"ename":"NameError","evalue":"name 'eng_spa' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"markdown","source":"**b. Inicializar la capa de embeddings.**\n\nPara inicializar una capa de embeddings con pesos predefinidos se utiliza el argumento `weights`. Además, como no queremos que se modifiquen los pesos, marcamos el argumento `trainable` como `False`.\n\nSiguiendo con nuestro ejemplo, haríamos:","metadata":{"id":"tMMsDFOy7nbR"}},{"cell_type":"code","source":"from keras.layers import Embedding\n\n# Se podría experimentar con diferentes modelos, variando parámetros como 'embedding_vec_length' y 'max_text_length'\n# Al entrenar nuevos modelos, recordar:\n# - O bien modificar el nombre del modelo en el parámetro callback (variable 'model_path') de la clase 'fit'\n# - O bien (opción preferida), una vez finaliza el entrenamiento, renombrar el modelo almacenado según 'model_path'.\n\nembedding_vec_length = 300\nmax_text_length = 8\n\n#############################################\n# SOLUCIÓN                                  #\n#############################################\nembedding_layer = Embedding(len(eng_tokenizer.word_index) + 1,\n                            embedding_vec_length,\n                            weights=[embedding_matrix],\n                            input_length=max_text_length,\n                            trainable=False,\n                            mask_zero=True)\n","metadata":{"id":"QLjfzHAq7nbR","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T10:37:52.623757Z","iopub.execute_input":"2025-06-13T10:37:52.624035Z","iopub.status.idle":"2025-06-13T10:37:52.723594Z","shell.execute_reply.started":"2025-06-13T10:37:52.624016Z","shell.execute_reply":"2025-06-13T10:37:52.722906Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"**c. Definición del nuevo modelo considerando los pesos del modelo preentrenado.**\n\n Implementa y entrena de nuevo un modelo de traducción automática del **idioma origen** al **idioma destino**, esta vez, cargando los pesos de la capa embedding a partir del modelo Glove preentrenado en inglés y disponible en `glove.42B.300d.txt`.\n","metadata":{"id":"2TAaf6I77nbR"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, RepeatVector, Dense\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint\nimport numpy as np\n\n# Parámetros\nmax_text_length      = 12\nembedding_vec_length = 300\nunits                = 256\neng_vocab_size       = len(eng_tokenizer.word_index) + 1\nspa_vocab_size       = len(spa_tokenizer.word_index) + 1\n\n# RECORTAMOS trainY y testY a max_text_length pasos y los reshapeamos\ntrainY_cut = trainY[:, :max_text_length]\ntestY_cut  = testY[:,  :max_text_length]\n\ny_train = trainY_cut.reshape(-1, max_text_length, 1)\ny_test  = testY_cut.reshape(-1,  max_text_length, 1)\n\n# Definimos el modelo\nmt_model = Sequential()\nmt_model.add(Embedding(\n    input_dim    = eng_vocab_size,\n    output_dim   = embedding_vec_length,\n    input_length = max_text_length,\n    weights      = [embedding_matrix],  # pesos GloVe preentrenados\n    trainable    = False,\n    mask_zero    = True\n))\nmt_model.add(LSTM(units))\nmt_model.add(RepeatVector(max_text_length))\nmt_model.add(LSTM(units, return_sequences=True))\nmt_model.add(Dense(spa_vocab_size, activation='softmax'))\n\n# Compilamos\nrms = optimizers.RMSprop(learning_rate=0.001)\nmt_model.compile(\n    optimizer = rms,\n    loss      = 'sparse_categorical_crossentropy',\n    metrics   = ['accuracy']\n)","metadata":{"id":"9hKTQCdp7nbS","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T11:04:33.603699Z","iopub.execute_input":"2025-06-13T11:04:33.604195Z","iopub.status.idle":"2025-06-13T11:04:33.707796Z","shell.execute_reply.started":"2025-06-13T11:04:33.604172Z","shell.execute_reply":"2025-06-13T11:04:33.707267Z"}},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":"### 1.2.3 Entrenamiento del modelo","metadata":{"id":"r9y3M34Yfm5G"}},{"cell_type":"markdown","source":"**Entrenar y guardar el modelo.**\n\nAunque este entrenamiento es quizá un \"poco\" más liviano que el anterior, recomendamos el uso de GPU si es viable.\n\n**Sugerencias:**\n\n- Probar con diferentes valores en *batch_size*. En el notebook de ejemplo se trabajó bien con el valor de 128.\n\n- Observar cómo evoluciona el modelo conforme se ejecuta cada *epoch*; en caso de no observar mejoras, se puede bajar su valor. En el notebook de ejemplo, se bajó el valor a 50 porque a partir de la *epoch* 32 no se notaron diferencias.\n\n- Revisar el `Notebook de Ejemplo`, subsección 1.2.3.2, en el que proporcionan pautas y guías para llevar un mejor control de las ejecuciones cuando ocurren reinicios de sesión o saturación de memoria.","metadata":{"id":"PqyHV6m3rIUn"}},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\n\n# entrenamos y guardamos el modelo\n\n# Callback de checkpoint\ncheckpoint = ModelCheckpoint(\n    'model_ta_eng_spa_glove.keras',\n    monitor        = 'val_loss',\n    save_best_only = True,\n    mode           = 'min',\n    verbose        = 1\n)\n\n# Entrenamos\nhistory = mt_model.fit(\n    trainX,\n    y_train,\n    epochs           = 30,\n    batch_size       = 256,\n    validation_split = 0.2,\n    callbacks        = [checkpoint],\n    verbose          = 1\n)\n\n# Evaluamos\nloss, acc = mt_model.evaluate(\n    testX,\n    y_test,\n    batch_size = 256,\n    verbose    = 1\n)\nprint(f\"loss: {loss:.4f}\")\nprint(f\"accuracy: {acc:.4f}\")","metadata":{"id":"l58sRZaY7nbS","scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T11:05:16.717057Z","iopub.execute_input":"2025-06-13T11:05:16.717357Z","iopub.status.idle":"2025-06-13T11:32:52.214706Z","shell.execute_reply.started":"2025-06-13T11:05:16.717337Z","shell.execute_reply":"2025-06-13T11:32:52.214121Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.2443 - loss: 6.7331\nEpoch 1: val_loss improved from inf to 5.65556, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 332ms/step - accuracy: 0.2444 - loss: 6.7236 - val_accuracy: 0.2528 - val_loss: 5.6556\nEpoch 2/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - accuracy: 0.2565 - loss: 5.4352\nEpoch 2: val_loss improved from 5.65556 to 5.47389, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 319ms/step - accuracy: 0.2566 - loss: 5.4346 - val_accuracy: 0.2719 - val_loss: 5.4739\nEpoch 3/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.2706 - loss: 5.2550\nEpoch 3: val_loss improved from 5.47389 to 5.18651, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 323ms/step - accuracy: 0.2706 - loss: 5.2546 - val_accuracy: 0.2710 - val_loss: 5.1865\nEpoch 4/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.2749 - loss: 5.1630\nEpoch 4: val_loss improved from 5.18651 to 5.13898, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 321ms/step - accuracy: 0.2749 - loss: 5.1627 - val_accuracy: 0.2738 - val_loss: 5.1390\nEpoch 5/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.2804 - loss: 5.0971\nEpoch 5: val_loss did not improve from 5.13898\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 320ms/step - accuracy: 0.2804 - loss: 5.0969 - val_accuracy: 0.2733 - val_loss: 5.6236\nEpoch 6/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.2863 - loss: 5.0532\nEpoch 6: val_loss did not improve from 5.13898\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 320ms/step - accuracy: 0.2863 - loss: 5.0531 - val_accuracy: 0.2768 - val_loss: 5.2326\nEpoch 7/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.2883 - loss: 5.0154\nEpoch 7: val_loss improved from 5.13898 to 5.07485, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 323ms/step - accuracy: 0.2883 - loss: 5.0154 - val_accuracy: 0.2794 - val_loss: 5.0749\nEpoch 8/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.2887 - loss: 5.0017\nEpoch 8: val_loss did not improve from 5.07485\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 319ms/step - accuracy: 0.2887 - loss: 5.0016 - val_accuracy: 0.2807 - val_loss: 5.1487\nEpoch 9/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.2922 - loss: 4.9556\nEpoch 9: val_loss did not improve from 5.07485\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 319ms/step - accuracy: 0.2922 - loss: 4.9556 - val_accuracy: 0.2827 - val_loss: 5.0914\nEpoch 10/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.2940 - loss: 4.9326\nEpoch 10: val_loss did not improve from 5.07485\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 320ms/step - accuracy: 0.2940 - loss: 4.9325 - val_accuracy: 0.2757 - val_loss: 5.0880\nEpoch 11/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.2963 - loss: 4.9023\nEpoch 11: val_loss did not improve from 5.07485\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 319ms/step - accuracy: 0.2964 - loss: 4.9022 - val_accuracy: 0.1493 - val_loss: 5.5676\nEpoch 12/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 0.2946 - loss: 4.8843\nEpoch 12: val_loss improved from 5.07485 to 5.00426, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 320ms/step - accuracy: 0.2946 - loss: 4.8841 - val_accuracy: 0.2768 - val_loss: 5.0043\nEpoch 13/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.3024 - loss: 4.8415\nEpoch 13: val_loss improved from 5.00426 to 4.93075, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 323ms/step - accuracy: 0.3024 - loss: 4.8415 - val_accuracy: 0.2982 - val_loss: 4.9308\nEpoch 14/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.3078 - loss: 4.8116\nEpoch 14: val_loss improved from 4.93075 to 4.92139, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 321ms/step - accuracy: 0.3078 - loss: 4.8116 - val_accuracy: 0.2856 - val_loss: 4.9214\nEpoch 15/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.3125 - loss: 4.7853\nEpoch 15: val_loss improved from 4.92139 to 4.81865, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 322ms/step - accuracy: 0.3126 - loss: 4.7851 - val_accuracy: 0.3101 - val_loss: 4.8186\nEpoch 16/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.3174 - loss: 4.7269\nEpoch 16: val_loss did not improve from 4.81865\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 318ms/step - accuracy: 0.3174 - loss: 4.7269 - val_accuracy: 0.2403 - val_loss: 5.0851\nEpoch 17/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.3182 - loss: 4.7030\nEpoch 17: val_loss did not improve from 4.81865\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 318ms/step - accuracy: 0.3182 - loss: 4.7030 - val_accuracy: 0.3017 - val_loss: 5.0423\nEpoch 18/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.3221 - loss: 4.6764\nEpoch 18: val_loss did not improve from 4.81865\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 319ms/step - accuracy: 0.3221 - loss: 4.6763 - val_accuracy: 0.3158 - val_loss: 4.8384\nEpoch 19/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.3287 - loss: 4.6190\nEpoch 19: val_loss did not improve from 4.81865\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 319ms/step - accuracy: 0.3287 - loss: 4.6191 - val_accuracy: 0.3137 - val_loss: 4.8987\nEpoch 20/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 288ms/step - accuracy: 0.3307 - loss: 4.5959\nEpoch 20: val_loss improved from 4.81865 to 4.72794, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 323ms/step - accuracy: 0.3307 - loss: 4.5959 - val_accuracy: 0.3231 - val_loss: 4.7279\nEpoch 21/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.3352 - loss: 4.5640\nEpoch 21: val_loss improved from 4.72794 to 4.71809, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 321ms/step - accuracy: 0.3352 - loss: 4.5639 - val_accuracy: 0.3082 - val_loss: 4.7181\nEpoch 22/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.3380 - loss: 4.5272\nEpoch 22: val_loss did not improve from 4.71809\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 319ms/step - accuracy: 0.3380 - loss: 4.5271 - val_accuracy: 0.2540 - val_loss: 4.9891\nEpoch 23/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.3413 - loss: 4.4856\nEpoch 23: val_loss did not improve from 4.71809\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 320ms/step - accuracy: 0.3413 - loss: 4.4855 - val_accuracy: 0.2721 - val_loss: 4.9128\nEpoch 24/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.3474 - loss: 4.4450\nEpoch 24: val_loss did not improve from 4.71809\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 318ms/step - accuracy: 0.3475 - loss: 4.4450 - val_accuracy: 0.3265 - val_loss: 4.7409\nEpoch 25/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.3527 - loss: 4.4100\nEpoch 25: val_loss improved from 4.71809 to 4.56526, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 321ms/step - accuracy: 0.3527 - loss: 4.4099 - val_accuracy: 0.3407 - val_loss: 4.5653\nEpoch 26/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.3569 - loss: 4.3711\nEpoch 26: val_loss improved from 4.56526 to 4.55862, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 321ms/step - accuracy: 0.3569 - loss: 4.3710 - val_accuracy: 0.3420 - val_loss: 4.5586\nEpoch 27/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.3631 - loss: 4.3185\nEpoch 27: val_loss did not improve from 4.55862\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 319ms/step - accuracy: 0.3631 - loss: 4.3185 - val_accuracy: 0.3361 - val_loss: 4.8799\nEpoch 28/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.3650 - loss: 4.3067\nEpoch 28: val_loss improved from 4.55862 to 4.53855, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 321ms/step - accuracy: 0.3650 - loss: 4.3065 - val_accuracy: 0.3301 - val_loss: 4.5386\nEpoch 29/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 0.3663 - loss: 4.2641\nEpoch 29: val_loss improved from 4.53855 to 4.42177, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 320ms/step - accuracy: 0.3664 - loss: 4.2639 - val_accuracy: 0.3557 - val_loss: 4.4218\nEpoch 30/30\n\u001b[1m170/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 287ms/step - accuracy: 0.3740 - loss: 4.2021\nEpoch 30: val_loss improved from 4.42177 to 4.36979, saving model to model_ta_eng_spa_glove.keras\n\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 321ms/step - accuracy: 0.3740 - loss: 4.2021 - val_accuracy: 0.3578 - val_loss: 4.3698\n\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 134ms/step - accuracy: 0.3555 - loss: 4.4073\nloss: 4.3988\naccuracy: 0.3563\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"### 1.2.4 Generación de predicciones\n\nEn este paso, aplicar el modelo para generar las predicciones utilizando el dataset de test.\n\n**Resultado esperado:**\n\n- Visualizar tabla de resultados: frase en idioma origen, texto traducido real vs. texto traducido generado.\n\n**Sugerencia:** Si durante la ejecución, se cancela la predicción por agotar toda la memoria disponible, recargar el modelo desde local (ver apartado 1.2.3.2 del `Notebook de Ejemplo`) y predecir solo para un subconjunto del fichero de test (aunque en este caso no serán validas las magnitudes de medición de calidad del sistema).   ","metadata":{"id":"wW-2Cnuqrbsu"}},{"cell_type":"code","source":"# Aplicamos el modelo y visualizamos resultados\n\n# Si debido a la configuración elegida, no se puede realizar la predicción por problemas de memoria\n# con todo el fichero de testX, utilizar un subconjunto del mismo\nreducir_test = True # Colocar True para elegir las primeras 100 filas.\nif reducir_test:\n    testX_to_pred = testX[:100]\nelse:\n    testX_to_pred = testX\n\n\n#############################################\n# SOLUCIÓN                                  #\n#############################################\n\nmodel_path = 'model_ta_eng_spa_glove.keras'\n\nmodel = load_model(model_path, compile=False)\nprobs = model.predict(testX_to_pred, batch_size=256, verbose=1)\npreds = np.argmax(probs, axis=-1)\n\nprint(\"Preds shape:\", preds.shape)\nprint(preds[:2])","metadata":{"id":"efztL7bg7nbT","trusted":true,"execution":{"iopub.status.busy":"2025-06-13T11:35:58.304596Z","iopub.execute_input":"2025-06-13T11:35:58.304877Z","iopub.status.idle":"2025-06-13T11:35:59.270437Z","shell.execute_reply.started":"2025-06-13T11:35:58.304855Z","shell.execute_reply":"2025-06-13T11:35:59.269773Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step\nPreds shape: (100, 8)\n[[ 1  6  6  6  6  0  0  0]\n [23 14  8  0  0  0  0  0]]\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"#############################################\n# SOLUCIÓN                                  #\n#############################################\n\n# Visualizamos resultados\ndef get_word(n, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == n:\n            return word\n    return None\n\npreds_text = []\n\nfor i in preds:\n    temp = []\n    for j in range(len(i)):\n        #Obtener la palabra que corresponde al índice del vocabulario de la lengua destino\n        t = get_word(i[j], spa_tokenizer)\n        if j > 0:\n            if (t == get_word(i[j-1], spa_tokenizer)) or (t == None):\n                     temp.append('')\n            else:\n                     temp.append(t)\n        else:\n            if(t == None):\n                temp.append('')\n            else:\n                temp.append(t) \n\n    preds_text.append(' '.join(temp))\n\nimport pandas as pd\n\npred_df = pd.DataFrame({'actual' : test[:100,0], 'predicted' : preds_text})\n\npred_df.sample(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-13T11:38:08.317709Z","iopub.execute_input":"2025-06-13T11:38:08.318302Z","iopub.status.idle":"2025-06-13T11:38:09.264667Z","shell.execute_reply.started":"2025-06-13T11:38:08.318280Z","shell.execute_reply":"2025-06-13T11:38:09.264022Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"                                               actual          predicted\n52                                do you remember us       él no  es    \n62                        let's drink to his success   la los que de    \n75                             we should follow them           no       \n1                              she called him a liar     las los es     \n0                              tom's wife's pregnant         de el      \n32                    he hasn't cleaned her room yet   un padre a es    \n89                             put it onto the table       la es  de    \n20                  it is no use asking me for money      en el que     \n11                                     rest in peace    la no que es    \n78  tom doesn't know the difference between viking...        de mi      ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>actual</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>52</th>\n      <td>do you remember us</td>\n      <td>él no  es</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>let's drink to his success</td>\n      <td>la los que de</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>we should follow them</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>she called him a liar</td>\n      <td>las los es</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>tom's wife's pregnant</td>\n      <td>de el</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>he hasn't cleaned her room yet</td>\n      <td>un padre a es</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>put it onto the table</td>\n      <td>la es  de</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>it is no use asking me for money</td>\n      <td>en el que</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>rest in peace</td>\n      <td>la no que es</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>tom doesn't know the difference between viking...</td>\n      <td>de mi</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":56}]}